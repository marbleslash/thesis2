{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rouge\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "#Plain text parsers since we are parsing through text\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "\n",
    "#for tokenization\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "sums = pd.read_csv('thesis_complaint_narratives_summaries.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SumBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.sum_basic import SumBasicSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.nlp.stemmers import Stemmer \n",
    "import math\n",
    "\n",
    "summarizer_w_stops = SumBasicSummarizer()\n",
    "summarizer_w_stops_stemming = SumBasicSummarizer(Stemmer('english'))\n",
    "summarizer_w_stops.stop_words = get_stop_words('english')\n",
    "summarizer_w_stops_stemming.stop_words = get_stop_words('english')\n",
    "summarizer_w_stemming = SumBasicSummarizer(Stemmer('english'))\n",
    "summarizer_only_stemming = SumBasicSummarizer(Stemmer('english'))\n",
    "\n",
    "summarizer = SumBasicSummarizer()\n",
    "file = sums['complaint_text'][0]\n",
    "\n",
    "## With Stop Words and No Stemming\n",
    "summaries1 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = summarizer(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    summaries1.append(sentence)\n",
    "\n",
    "# No Stop Words\n",
    "summaries2 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = summarizer_w_stops(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    summaries2.append(sentence)\n",
    "\n",
    "summaries3 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = summarizer_w_stops_stemming(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    summaries3.append(sentence)\n",
    "\n",
    "summaries4 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = summarizer_only_stemming(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    summaries4.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.27262686020176663, 'p': 0.21902773372030268, 'r': 0.4185506483991545}, 'rouge-2': {'f': 0.07595604621747407, 'p': 0.061324457543728214, 'r': 0.11744391302200484}, 'rouge-l': {'f': 0.17594054004042026, 'p': 0.14203171356798108, 'r': 0.27037937440658394}}\n",
      "{'rouge-1': {'f': 0.2774361312841741, 'p': 0.22195647149889208, 'r': 0.42861151953252713}, 'rouge-2': {'f': 0.06785762186534543, 'p': 0.054412903080743355, 'r': 0.10585720436472254}, 'rouge-l': {'f': 0.1699241580524412, 'p': 0.1358955564392772, 'r': 0.2641179633873929}}\n",
      "{'rouge-1': {'f': 0.2774361312841741, 'p': 0.22195647149889208, 'r': 0.42861151953252713}, 'rouge-2': {'f': 0.06785762186534543, 'p': 0.054412903080743355, 'r': 0.10585720436472254}, 'rouge-l': {'f': 0.1699241580524412, 'p': 0.1358955564392772, 'r': 0.2641179633873929}}\n",
      "{'rouge-1': {'f': 0.27262686020176663, 'p': 0.21902773372030268, 'r': 0.4185506483991545}, 'rouge-2': {'f': 0.07595604621747407, 'p': 0.061324457543728214, 'r': 0.11744391302200484}, 'rouge-l': {'f': 0.17594054004042026, 'p': 0.14203171356798108, 'r': 0.27037937440658394}}\n"
     ]
    }
   ],
   "source": [
    "# Sumbasic1\n",
    "real_sums = list(sums['complaint_summary'])\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],\n",
    "                           max_n=2,\n",
    "                           apply_avg = True,\n",
    "                           stemming=True)\n",
    "\n",
    "n = (range(26, 50))\n",
    "\n",
    "sumbasic1_scores = evaluator.get_scores(summaries1[0:50], real_sums[0:50])\n",
    "\n",
    "# Sumbasic2\n",
    "sumbasic2_scores = evaluator.get_scores(summaries2[0:50], real_sums[0:50])\n",
    "\n",
    "# Sumbasic3\n",
    "sumbasic3_scores = evaluator.get_scores(summaries3[0:50], real_sums[0:50])\n",
    "\n",
    "# Sumbasic3\n",
    "sumbasic4_scores = evaluator.get_scores(summaries4[0:50], real_sums[0:50])\n",
    "\n",
    "print(sumbasic1_scores)\n",
    "print(sumbasic2_scores)\n",
    "print(sumbasic3_scores)\n",
    "print(sumbasic4_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "\n",
    "tsummarizer_w_stops = TextRankSummarizer()\n",
    "tsummarizer_w_stops_stemming = TextRankSummarizer(Stemmer('english'))\n",
    "tsummarizer_w_stops.stop_words = get_stop_words('english')\n",
    "tsummarizer_w_stops_stemming.stop_words = get_stop_words('english')\n",
    "tsummarizer_w_stemming = TextRankSummarizer(Stemmer('english'))\n",
    "tsummarizer_only_stemming = TextRankSummarizer(Stemmer('english'))\n",
    "\n",
    "tsummarizer = TextRankSummarizer()\n",
    "file = sums['complaint_text'][0]\n",
    "\n",
    "## With Stop Words and No Stemming\n",
    "tsummaries1 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = tsummarizer(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    tsummaries1.append(sentence)\n",
    "\n",
    "# No Stop Words\n",
    "tsummaries2 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = tsummarizer_w_stops(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    tsummaries2.append(sentence)\n",
    "\n",
    "tsummaries3 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = tsummarizer_w_stops_stemming(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    tsummaries3.append(sentence)\n",
    "    \n",
    "tsummaries4 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = tsummarizer_only_stemming(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    tsummaries4.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.27262686020176663, 'p': 0.21902773372030268, 'r': 0.4185506483991545}, 'rouge-2': {'f': 0.07595604621747407, 'p': 0.061324457543728214, 'r': 0.11744391302200484}, 'rouge-l': {'f': 0.17594054004042026, 'p': 0.14203171356798108, 'r': 0.27037937440658394}}\n",
      "{'rouge-1': {'f': 0.2774361312841741, 'p': 0.22195647149889208, 'r': 0.42861151953252713}, 'rouge-2': {'f': 0.06785762186534543, 'p': 0.054412903080743355, 'r': 0.10585720436472254}, 'rouge-l': {'f': 0.1699241580524412, 'p': 0.1358955564392772, 'r': 0.2641179633873929}}\n",
      "{'rouge-1': {'f': 0.2774361312841741, 'p': 0.22195647149889208, 'r': 0.42861151953252713}, 'rouge-2': {'f': 0.06785762186534543, 'p': 0.054412903080743355, 'r': 0.10585720436472254}, 'rouge-l': {'f': 0.1699241580524412, 'p': 0.1358955564392772, 'r': 0.2641179633873929}}\n",
      "{'rouge-1': {'f': 0.27262686020176663, 'p': 0.21902773372030268, 'r': 0.4185506483991545}, 'rouge-2': {'f': 0.07595604621747407, 'p': 0.061324457543728214, 'r': 0.11744391302200484}, 'rouge-l': {'f': 0.17594054004042026, 'p': 0.14203171356798108, 'r': 0.27037937440658394}}\n"
     ]
    }
   ],
   "source": [
    "# TextRank1 (No Stemming and Keeping Stop Words)\n",
    "real_sums = list(sums['complaint_summary'])\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],\n",
    "                           max_n=2,\n",
    "                           apply_avg = True,\n",
    "                           stemming=True)\n",
    "\n",
    "t1_scores = evaluator.get_scores(tsummaries1[0:50], real_sums[0:50])\n",
    "\n",
    "# TextRank2 (Only Stop Words)\n",
    "t2_scores = evaluator.get_scores(tsummaries2[0:50], real_sums[0:50])\n",
    "\n",
    "# TextRank3 (Stemming plus Stop Words)\n",
    "t3_scores = evaluator.get_scores(tsummaries3[0:50], real_sums[0:50])\n",
    "\n",
    "# TextRank4 (Only Stemming)\n",
    "t4_scores = evaluator.get_scores(tsummaries4[0:50], real_sums[0:50])\n",
    "\n",
    "print(sumbasic1_scores)\n",
    "print(sumbasic2_scores)\n",
    "print(sumbasic3_scores)\n",
    "print(sumbasic4_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "\n",
    "lsummarizer_w_stops = LexRankSummarizer()\n",
    "lsummarizer_w_stops_stemming = LexRankSummarizer(Stemmer('english'))\n",
    "lsummarizer_w_stops.stop_words = get_stop_words('english')\n",
    "lsummarizer_w_stops_stemming.stop_words = get_stop_words('english')\n",
    "lsummarizer_w_stemming = LexRankSummarizer(Stemmer('english'))\n",
    "lsummarizer_only_stemming = LexRankSummarizer(Stemmer('english'))\n",
    "\n",
    "lsummarizer = LexRankSummarizer()\n",
    "file = sums['complaint_text'][0]\n",
    "\n",
    "## With Stop Words and No Stemming\n",
    "lsummaries1 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = lsummarizer(parser.document, math.ceil(sent_length * 0.50))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    lsummaries1.append(sentence)\n",
    "\n",
    "# No Stop Words\n",
    "lsummaries2 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = lsummarizer_w_stops(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    lsummaries2.append(sentence)\n",
    "\n",
    "lsummaries3 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = lsummarizer_w_stops_stemming(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    lsummaries3.append(sentence)\n",
    "    \n",
    "lsummaries4 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = lsummarizer_only_stemming(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    lsummaries4.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.27528452194602876, 'p': 0.21748603326431337, 'r': 0.43180897667303486}, 'rouge-2': {'f': 0.07507518095617419, 'p': 0.05941747183246172, 'r': 0.11646319136193259}, 'rouge-l': {'f': 0.17651776363675672, 'p': 0.138947206609378, 'r': 0.27900431945444165}}\n",
      "{'rouge-1': {'f': 0.27123903520855813, 'p': 0.21469664029783533, 'r': 0.4271062342674714}, 'rouge-2': {'f': 0.07426550768181774, 'p': 0.058855822576608104, 'r': 0.11637491061037211}, 'rouge-l': {'f': 0.17191436356550122, 'p': 0.13514696743089522, 'r': 0.2754814413656501}}\n",
      "{'rouge-1': {'f': 0.26799347317782585, 'p': 0.21145627433024308, 'r': 0.4255153139287341}, 'rouge-2': {'f': 0.0702345817201961, 'p': 0.055495706565309534, 'r': 0.11198975908573484}, 'rouge-l': {'f': 0.1682897288471954, 'p': 0.13185843967438213, 'r': 0.2719090086568839}}\n",
      "{'rouge-1': {'f': 0.2697128761309085, 'p': 0.21386237801715616, 'r': 0.42216106722324825}, 'rouge-2': {'f': 0.07107808135774137, 'p': 0.056470128971742, 'r': 0.1114433228749469}, 'rouge-l': {'f': 0.17149239957633103, 'p': 0.13567572274248219, 'r': 0.270608452115123}}\n"
     ]
    }
   ],
   "source": [
    "# LexRank1 (No Stemming and Keeping Stop Words)\n",
    "real_sums = list(sums['complaint_summary'])\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],\n",
    "                           max_n=2,\n",
    "                           apply_avg = True,\n",
    "                           stemming=True)\n",
    "\n",
    "lexrank1_scores = evaluator.get_scores(lsummaries1[0:50], real_sums[0:50])\n",
    "\n",
    "# LexRank2 (Only Stop Words)\n",
    "lexrank2_scores = evaluator.get_scores(lsummaries2[0:50], real_sums[0:50])\n",
    "\n",
    "# LexRank3 (Stemming plus Stop Words)\n",
    "lexrank3_scores = evaluator.get_scores(lsummaries3[0:50], real_sums[0:50])\n",
    "\n",
    "# LexRank4 (Only Stemming)\n",
    "lexrank4_scores = evaluator.get_scores(lsummaries4[0:50], real_sums[0:50])\n",
    "\n",
    "print(lexrank1_scores)\n",
    "print(lexrank2_scores)\n",
    "print(lexrank3_scores)\n",
    "print(lexrank4_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "\n",
    "l2summarizer_w_stops = LsaSummarizer()\n",
    "l2summarizer_w_stops_stemming = LsaSummarizer(Stemmer('english'))\n",
    "l2summarizer_w_stops.stop_words = get_stop_words('english')\n",
    "l2summarizer_w_stops_stemming.stop_words = get_stop_words('english')\n",
    "l2summarizer_w_stemming = LsaSummarizer(Stemmer('english'))\n",
    "l2summarizer_only_stemming = LsaSummarizer(Stemmer('english'))\n",
    "\n",
    "l2summarizer = LsaSummarizer()\n",
    "file = sums['complaint_text'][0]\n",
    "\n",
    "## With Stop Words and No Stemming\n",
    "l2summaries1 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = l2summarizer(parser.document, math.ceil(sent_length * .50))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    l2summaries1.append(sentence)\n",
    "\n",
    "# No Stop Words\n",
    "l2summaries2 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = l2summarizer_w_stops(parser.document, math.ceil(sent_length/2))\n",
    "    #print(summary)\n",
    "    sentence = \"\"\n",
    "    \n",
    "    for sent in summary:\n",
    "        #print(sent)\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    l2summaries2.append(sentence)\n",
    "\n",
    "l2summaries3 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = l2summarizer_w_stops_stemming(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    l2summaries3.append(sentence)\n",
    "    \n",
    "l2summaries4 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = l2summarizer_only_stemming(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    l2summaries4.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.2883475463244471, 'p': 0.22651045648265147, 'r': 0.4622577513519083}, 'rouge-2': {'f': 0.07888087046624294, 'p': 0.06228649269674934, 'r': 0.1255541577525107}, 'rouge-l': {'f': 0.18348824255678106, 'p': 0.1430078508602336, 'r': 0.2992796280677645}}\n",
      "{'rouge-1': {'f': 0.285126573082273, 'p': 0.22388999067854878, 'r': 0.4585602308680841}, 'rouge-2': {'f': 0.0814654478371896, 'p': 0.06488453418566936, 'r': 0.12866521341554554}, 'rouge-l': {'f': 0.18072728434232332, 'p': 0.14131102361293418, 'r': 0.29470633101858856}}\n",
      "{'rouge-1': {'f': 0.2822749850473065, 'p': 0.22283315813071805, 'r': 0.44906931081410034}, 'rouge-2': {'f': 0.07925017648243601, 'p': 0.06363507464450192, 'r': 0.12344339738100979}, 'rouge-l': {'f': 0.17797123503871579, 'p': 0.14009232041812675, 'r': 0.2863852151836046}}\n",
      "{'rouge-1': {'f': 0.2874283551057217, 'p': 0.2260776786486453, 'r': 0.46043471550141246}, 'rouge-2': {'f': 0.07848785070628166, 'p': 0.062226846501557055, 'r': 0.1243589042465776}, 'rouge-l': {'f': 0.18209715460474304, 'p': 0.14218345314986633, 'r': 0.29667093241559056}}\n"
     ]
    }
   ],
   "source": [
    "# LSA1 (No Stemming and Keeping Stop Words)\n",
    "real_sums = list(sums['complaint_summary'])\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],\n",
    "                           max_n=2,\n",
    "                           apply_avg = True,\n",
    "                           stemming=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lsa1_scores = evaluator.get_scores(l2summaries1[0:50], real_sums[0:50])\n",
    "\n",
    "#LSA2 (Removing Only Stop Words)\n",
    "lsa2_scores = evaluator.get_scores(l2summaries2[0:50], real_sums[0:50])\n",
    "\n",
    "# LSA3 (Stemming plus Removing Stop Words)\n",
    "lsa3_scores = evaluator.get_scores(l2summaries3[0:50], real_sums[0:50])\n",
    "\n",
    "# LSA4 (Only Stemming)\n",
    "lsa4_scores = evaluator.get_scores(l2summaries4[0:50], real_sums[0:50])\n",
    "\n",
    "print(lsa1_scores)\n",
    "print(lsa2_scores)\n",
    "print(lsa3_scores)\n",
    "print(lsa4_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL-Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.kl import KLSummarizer\n",
    "\n",
    "\n",
    "klsummarizer_w_stops = KLSummarizer()\n",
    "klsummarizer_w_stops_stemming = KLSummarizer(Stemmer('english'))\n",
    "klsummarizer_w_stops.stop_words = get_stop_words('english')\n",
    "klsummarizer_w_stops_stemming.stop_words = get_stop_words('english')\n",
    "klsummarizer_w_stemming = KLSummarizer(Stemmer('english'))\n",
    "klsummarizer_only_stemming = KLSummarizer(Stemmer('english'))\n",
    "\n",
    "klsummarizer = KLSummarizer()\n",
    "file = sums['complaint_text'][0]\n",
    "\n",
    "## With Stop Words and No Stemming\n",
    "klsummaries1 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = klsummarizer(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    klsummaries1.append(sentence)\n",
    "\n",
    "# No Stop Words\n",
    "klsummaries2 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = klsummarizer_w_stops(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    klsummaries2.append(sentence)\n",
    "\n",
    "klsummaries3 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = klsummarizer_w_stops_stemming(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    klsummaries3.append(sentence)\n",
    "    \n",
    "klsummaries4 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = klsummarizer_only_stemming(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    klsummaries4.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.25703606214316943, 'p': 0.215649191238388, 'r': 0.37277670115104355}, 'rouge-2': {'f': 0.06026429614394301, 'p': 0.051082669621185614, 'r': 0.0859634853296526}, 'rouge-l': {'f': 0.16181865280187233, 'p': 0.13635085864500562, 'r': 0.23593988084125045}}\n",
      "{'rouge-1': {'f': 0.2538757431531133, 'p': 0.20316078164745494, 'r': 0.38856413214807267}, 'rouge-2': {'f': 0.060540092860218636, 'p': 0.04882619268115586, 'r': 0.09146848586245206}, 'rouge-l': {'f': 0.15774941155836247, 'p': 0.12566308447003632, 'r': 0.24307250728101898}}\n",
      "{'rouge-1': {'f': 0.2538757431531133, 'p': 0.20316078164745494, 'r': 0.38856413214807267}, 'rouge-2': {'f': 0.060540092860218636, 'p': 0.04882619268115586, 'r': 0.09146848586245206}, 'rouge-l': {'f': 0.15774941155836247, 'p': 0.12566308447003632, 'r': 0.24307250728101898}}\n",
      "{'rouge-1': {'f': 0.25703606214316943, 'p': 0.215649191238388, 'r': 0.37277670115104355}, 'rouge-2': {'f': 0.06026429614394301, 'p': 0.051082669621185614, 'r': 0.0859634853296526}, 'rouge-l': {'f': 0.16181865280187233, 'p': 0.13635085864500562, 'r': 0.23593988084125045}}\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.kl import KLSummarizer\n",
    "\n",
    "# LexRank1 (No Stemming and Keeping Stop Words)\n",
    "real_sums = list(sums['complaint_summary'])\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],\n",
    "                           max_n=2,\n",
    "                           apply_avg = True,\n",
    "                           stemming=True)\n",
    "\n",
    "kl1_scores = evaluator.get_scores(klsummaries1[0:50], real_sums[0:50])\n",
    "\n",
    "# LexRank2 (Only Stop Words)\n",
    "kl2_scores = evaluator.get_scores(klsummaries2[0:50], real_sums[0:50])\n",
    "\n",
    "# LexRank3 (Stemming plus Stop Words)\n",
    "kl3_scores = evaluator.get_scores(klsummaries3[0:50], real_sums[0:50])\n",
    "\n",
    "# LexRank4 (Only Stemming)\n",
    "kl4_scores = evaluator.get_scores(klsummaries4[0:50], real_sums[0:50])\n",
    "\n",
    "print(kl1_scores)\n",
    "print(kl2_scores)\n",
    "print(kl3_scores)\n",
    "print(kl4_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA-Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.2941917399886499,\n",
       "  'p': 0.251090475544598,\n",
       "  'r': 0.4255154696197158},\n",
       " 'rouge-2': {'f': 0.07893464961153542,\n",
       "  'p': 0.06833999663498057,\n",
       "  'r': 0.11133502927174226},\n",
       " 'rouge-l': {'f': 0.18491702491302564,\n",
       "  'p': 0.15835704904125802,\n",
       "  'r': 0.2667074520165528}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mysumy.sumy.summarizers.LSAPlus_SumPlus import LSAPlus_SumPlus\n",
    "\n",
    "# Obtaining the trimmed text\n",
    "trimmed = LSAPlus_SumPlus(sums['complaint_text'])\n",
    "\n",
    "# Applying LSA on top of the trimmed text\n",
    "lsa_pro_sums = []\n",
    "lsa_summarizer = LsaSummarizer()\n",
    "for text in trimmed:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = lsa_summarizer(parser.document, math.ceil(sent_length * 0.50))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    lsa_pro_sums.append(sentence)    \n",
    "    \n",
    "    \n",
    "lsa_pro_scores = evaluator.get_scores(lsa_pro_sums[0:50], real_sums[0:50])\n",
    "lsa_pro_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Summarizer Implementation\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "entries = list(sums['complaint_text'])\n",
    "real_sums = list(sums['complaint_summary'])\n",
    "## Tokenizing the entries by sentence\n",
    "f_rouge1 = []\n",
    "f_rouge2 = []\n",
    "f_rougel = []\n",
    "for i in range(500):\n",
    "    tokenized_entries = []\n",
    "    sampled_sums = []\n",
    "    for entry in entries:\n",
    "        tokenized_entries.append(sent_tokenize(entry))\n",
    "\n",
    "\n",
    "    for entry in tokenized_entries[0:50]:\n",
    "        entry_length = len(entry)\n",
    "        total_sent = \"\"\n",
    "        sampled = random.sample(entry, math.ceil(entry_length/2))\n",
    "        for sent in sampled:\n",
    "            total_sent = total_sent + \" \" + sent\n",
    "        sampled_sums.append(total_sent)\n",
    "\n",
    "\n",
    "    scores_sampled_sums = evaluator.get_scores(sampled_sums, real_sums[0:50])\n",
    "    f_rouge1.append(scores_sampled_sums['rouge-1']['f'])\n",
    "    f_rouge2.append(scores_sampled_sums['rouge-2']['f'])\n",
    "    f_rougel.append(scores_sampled_sums['rouge-l']['f'])\n",
    "\n",
    "randoms = [np.mean(f_rouge1), np.mean(f_rouge2), np.mean(f_rougel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean Rouge Scores of Random Summarizer implemented 500 times\n",
    "randoms = [np.mean(f_rouge1), np.mean(f_rouge2), np.mean(f_rougel)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 12, Table 14, Table 24 in Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure 12 in Report: Taking the Best Performance for Each Summarizer and Putting it in the table\n",
    "\n",
    "sb_scores = (sumbasic2_scores['rouge-1']['f'], sumbasic2_scores['rouge-2']['f'], sumbasic2_scores['rouge-l']['f'])\n",
    "tr_scores = t1_scores['rouge-1']['f'], t1_scores['rouge-2']['f'], t1_scores['rouge-l']['f']\n",
    "lr_scores = lexrank1_scores['rouge-1']['f'], lexrank1_scores['rouge-2']['f'], lexrank1_scores['rouge-l']['f']\n",
    "lsa_scores = lsa1_scores['rouge-1']['f'], lsa1_scores['rouge-2']['f'], lsa1_scores['rouge-l']['f']\n",
    "kl_scores = kl1_scores['rouge-1']['f'], kl1_scores['rouge-2']['f'], kl1_scores['rouge-l']['f']\n",
    "lsa_pro_scores2 = lsa_pro_scores['rouge-1']['f'], lsa_pro_scores['rouge-2']['f'], lsa_pro_scores['rouge-l']['f']\n",
    "randoms = [np.mean(f_rouge1), np.mean(f_rouge2), np.mean(f_rougel)]\n",
    "\n",
    "d = [randoms, sb_scores, tr_scores, kl_scores,  lr_scores,\n",
    "     lsa_scores, lsa_pro_scores2]\n",
    "df = pd.DataFrame(data=d, columns = ['rouge-1 (%)','rouge-2 (%)', 'rouge-L (%)'], index = ['Random',\n",
    "                                                                                           'SumBasic', 'TextRank', 'KLSum', 'LexRank', 'LSA', 'LSAPro'])\n",
    "print('Rouge F Scores')\n",
    "round(df, 3) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtaining the indices of the text, and then breaking up the text by text Length\n",
    "\n",
    "doc_len = []\n",
    "count = 0\n",
    "for doc in (sums['complaint_text'][0:50]):\n",
    "\n",
    "    doc_len.append(len(sent_tokenize(doc)))\n",
    "\n",
    "len_array = np.array(doc_len)\n",
    "new_array = len_array\n",
    "\n",
    "short_indices = np.where(new_array < 10)[0]\n",
    "medium_indices = (new_array >= 10) & (new_array < 16)\n",
    "long_indices = np.where(new_array >=16)[0]\n",
    "\n",
    "short = new_array[short_indices]\n",
    "medium = new_array[medium_indices]\n",
    "long = new_array[long_indices]\n",
    "\n",
    "short_indices = np.where(new_array < 10)[0]\n",
    "medium_indices = np.where(((new_array >= 10) & (new_array < 16)) == True  )[0]\n",
    "long_indices = np.where(((new_array >= 16) & (new_array <= 20)) == True)[0]\n",
    "super_long_indices = np.where((new_array > 20) == True)[0]\n",
    "\n",
    "\n",
    "short = new_array[short_indices]\n",
    "medium = new_array[medium_indices]\n",
    "long = new_array[long_indices]\n",
    "super_long = new_array[super_long_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "## Fitting the Summarizers on data Separated by Text Length\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "# The original text at hand, to produce the relevant text summaries in the correct rder\n",
    "good_sums = list(sums[0:50]['complaint_summary'])\n",
    "short_sums = [good_sums[i] for i in short_indices]\n",
    "medium_sums = [good_sums[i] for i in medium_indices]\n",
    "long_sums = [good_sums[i] for i in long_indices]\n",
    "super_long_sums = [good_sums[i] for i in super_long_indices]\n",
    "\n",
    "\n",
    "#sumbasic\n",
    "sb_short = [summaries2[0:50][i] for i in short_indices]\n",
    "sb_med = [summaries2[0:50][i] for i in medium_indices]\n",
    "sb_long = [summaries2[0:50][i] for i in long_indices]\n",
    "sb_super_long = [summaries2[0:50][i] for i in super_long_indices]\n",
    "\n",
    "# TextRank\n",
    "tr_short = [tsummaries1[0:50][i] for i in short_indices]\n",
    "tr_med = [tsummaries1[0:50][i] for i in medium_indices]\n",
    "tr_long = [tsummaries1[0:50][i] for i in long_indices]\n",
    "tr_super_long = [tsummaries1[0:50][i] for i in super_long_indices]\n",
    "\n",
    "# KLSum\n",
    "l_short = [klsummaries1[0:50][i] for i in short_indices]\n",
    "l_med = [klsummaries1[0:50][i] for i in medium_indices]\n",
    "l_long = [klsummaries1[0:50][i] for i in long_indices]\n",
    "l_super_long = [klsummaries1[0:50][i] for i in super_long_indices]\n",
    "\n",
    "# Lexrank\n",
    "\n",
    "lex_short = [lsummaries1[0:50][i] for i in short_indices]\n",
    "lex_med = [lsummaries1[0:50][i] for i in medium_indices]\n",
    "lex_long = [lsummaries1[0:50][i] for i in long_indices]\n",
    "lex_super_long = [lsummaries1[0:50][i] for i in super_long_indices]\n",
    "\n",
    "\n",
    "#LSA\n",
    "lsa_short = [l2summaries1[0:50][i] for i in short_indices]\n",
    "lsa_med = [l2summaries1[0:50][i] for i in medium_indices]\n",
    "lsa_long = [l2summaries1[0:50][i] for i in long_indices]\n",
    "lsa_super_long = [l2summaries1[0:50][i] for i in super_long_indices]\n",
    "\n",
    "#LSA-PRO\n",
    "lpro_short = [lsa_pro_sums[0:50][i] for i in short_indices]\n",
    "lpro_med = [lsa_pro_sums[0:50][i] for i in medium_indices]\n",
    "lpro_long = [lsa_pro_sums[0:50][i] for i in long_indices]\n",
    "lpro_super_long = [lsa_pro_sums[0:50][i] for i in super_long_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "## Obtaining Rouge Scores for Short, Medium, Long, and Super Long Texts\n",
    "##################################################################################\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],\n",
    "                           max_n=2,\n",
    "                           apply_avg = True,\n",
    "                           stemming=True)\n",
    "\n",
    "\n",
    "sb1 = evaluator.get_scores(sb_short, short_sums)\n",
    "sb2 = evaluator.get_scores(sb_med, medium_sums)\n",
    "sb3 = evaluator.get_scores(sb_long, long_sums)\n",
    "sb4 = evaluator.get_scores(sb_super_long, super_long_sums)\n",
    "\n",
    "tr1 = evaluator.get_scores(tr_short, short_sums)\n",
    "tr2 = evaluator.get_scores(tr_med, medium_sums)\n",
    "tr3 = evaluator.get_scores(tr_long, long_sums)\n",
    "tr4 = evaluator.get_scores(tr_super_long, super_long_sums)\n",
    "\n",
    "kl1 = evaluator.get_scores(l_short, short_sums)\n",
    "kl2 = evaluator.get_scores(l_med, medium_sums)\n",
    "kl3 = evaluator.get_scores(l_long, long_sums)\n",
    "kl4 = evaluator.get_scores(l_super_long, super_long_sums)\n",
    "\n",
    "lex1 = evaluator.get_scores(lex_short, short_sums)\n",
    "lex2 = evaluator.get_scores(lex_med, medium_sums)\n",
    "lex3 = evaluator.get_scores(lex_long, long_sums)\n",
    "lex4 = evaluator.get_scores(lex_super_long, super_long_sums)\n",
    "\n",
    "lsa1 = evaluator.get_scores(lsa_short, short_sums)\n",
    "lsa2 = evaluator.get_scores(lsa_med, medium_sums)\n",
    "lsa3 = evaluator.get_scores(lsa_long, long_sums)\n",
    "lsa4 = evaluator.get_scores(lsa_super_long, super_long_sums)\n",
    "\n",
    "lpro1 = evaluator.get_scores(lpro_short, short_sums)\n",
    "lpro2 = evaluator.get_scores(lpro_med, medium_sums)\n",
    "lpro3 = evaluator.get_scores(lpro_long, long_sums)\n",
    "lpro4 = evaluator.get_scores(lpro_super_long, super_long_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sumbasic')\n",
    "print(sb1['rouge-1'])\n",
    "print(sb2['rouge-1'])\n",
    "print(sb3['rouge-1'])\n",
    "print(sb4['rouge-1'])\n",
    "print(\" \")\n",
    "print('textrank')\n",
    "print(tr1['rouge-1'])\n",
    "print(tr2['rouge-1'])\n",
    "print(tr3['rouge-1'])\n",
    "print(tr4['rouge-1'])\n",
    "print(' ')\n",
    "print('KL-Sum')\n",
    "print(kl1['rouge-1'])\n",
    "print(kl2['rouge-1'])\n",
    "print(kl3['rouge-1'])\n",
    "print(kl4['rouge-1'])\n",
    "\n",
    "print(' ')\n",
    "print('Lexrank')\n",
    "print(lex1['rouge-1'])\n",
    "print(lex2['rouge-1'])\n",
    "print(lex3['rouge-1'])\n",
    "print(lex4['rouge-1'])\n",
    "\n",
    "print(' ')\n",
    "print(\"LSA\")\n",
    "print(lsa1['rouge-1'])\n",
    "print(lsa2['rouge-1'])\n",
    "print(lsa3['rouge-1'])\n",
    "print(lsa4['rouge-1'])\n",
    "\n",
    "print(' ')\n",
    "print(\"LSA-Pro\")\n",
    "print(lpro1['rouge-1'])\n",
    "print(lpro2['rouge-1'])\n",
    "print(lpro3['rouge-1'])\n",
    "print(lpro4['rouge-1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 15 for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_by_length = [sb1['rouge-1']['f'], sb2['rouge-1']['f'], sb3['rouge-1']['f'], sb4['rouge-1']['f']]\n",
    "tr_by_length = [tr1['rouge-1']['f'], tr2['rouge-1']['f'], tr3['rouge-1']['f'], tr4['rouge-1']['f']]\n",
    "kl_by_length = [kl1['rouge-1']['f'], kl2['rouge-1']['f'], kl3['rouge-1']['f'], kl4['rouge-1']['f']]\n",
    "lex_by_length = [lex1['rouge-1']['f'], lex2['rouge-1']['f'], lex3['rouge-1']['f'], lex4['rouge-1']['f']]\n",
    "lsa_by_length = [lsa1['rouge-1']['f'], lsa2['rouge-1']['f'], lsa3['rouge-1']['f'], lsa4['rouge-1']['f']]\n",
    "lpro_by_length = [lpro1['rouge-1']['f'], lpro2['rouge-1']['f'], lpro3['rouge-1']['f'], lpro4['rouge-1']['f']]\n",
    "\n",
    "\n",
    "d2 = [sb_by_length, tr_by_length, kl_by_length, lex_by_length, lsa_by_length, lpro_by_length]\n",
    "df2 = pd.DataFrame(data=d2, columns = ['Short','Medium', 'Long', 'Super Long'], \n",
    "                   index = ['SumBasic', 'TextRank', 'KLSum', 'LexRank', 'LSA', 'LSA-Pro'])\n",
    "\n",
    "print(\"Rouge-1 F Scores (%) by Text Length\")\n",
    "round(df2, 3) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 16, Table 27 for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_rouge_short = lsa1['rouge-1']['f'], lsa1['rouge-1']['p'], lsa1['rouge-1']['r']\n",
    "lpro_rouge_short = lpro1['rouge-1']['f'], lpro1['rouge-1']['p'], lpro1['rouge-1']['r']\n",
    "\n",
    "d3 = [lsa_rouge_short, lpro_rouge_short]\n",
    "df3 = pd.DataFrame(data=d3, columns = ['F1-Score (%)', 'Precision (%)', 'Recall (%)'], \n",
    "                   index = ['LSA', 'LSA-Pro'])\n",
    "print('Comparing Rouge-1 for Short Summaries')\n",
    "round(df3, 3) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 17, Table 28 for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa2_rouge_short = lsa2['rouge-1']['f'], lsa2['rouge-1']['p'], lsa2['rouge-1']['r']\n",
    "lpro2_rouge_short = lpro2['rouge-1']['f'], lpro2['rouge-1']['p'], lpro2['rouge-1']['r']\n",
    "\n",
    "d4 = [lsa2_rouge_short, lpro2_rouge_short]\n",
    "df4 = pd.DataFrame(data=d4, columns = ['F1-Score (%)', 'Precision (%)', 'Recall (%)'], \n",
    "                   index = ['LSA', 'LSA-Pro'])\n",
    "print('Comparing Rouge-1 for Medium Summaries')\n",
    "round(df4, 3) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 18, Table 25 for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa3_rouge_short = lsa3['rouge-1']['f'], lsa3['rouge-1']['p'], lsa3['rouge-1']['r']\n",
    "lpro3_rouge_short = lpro3['rouge-1']['f'], lpro3['rouge-1']['p'], lpro3['rouge-1']['r']\n",
    "\n",
    "d5 = [lsa3_rouge_short, lpro3_rouge_short]\n",
    "df5 = pd.DataFrame(data=d5, columns = ['F1-Score (%)', 'Precision (%)', 'Recall (%)'], \n",
    "                   index = ['LSA', 'LSA-Pro'])\n",
    "print('Comparing Rouge-1 for Long Summaries')\n",
    "round(df5, 3) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 19, Table 26 for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa4_rouge_short = lsa4['rouge-1']['f'], lsa4['rouge-1']['p'], lsa4['rouge-1']['r']\n",
    "lpro4_rouge_short = lpro4['rouge-1']['f'], lpro4['rouge-1']['p'], lpro4['rouge-1']['r']\n",
    "\n",
    "d6 = [lsa4_rouge_short, lpro4_rouge_short]\n",
    "df6 = pd.DataFrame(data=d6, columns = ['F1-Score (%)', 'Precision (%)', 'Recall (%)'], \n",
    "                   index = ['LSA', 'LSA-Pro'])\n",
    "print('Comparing Rouge-1 for Super Long Summaries')\n",
    "round(df6, 3) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 20, Obtaining the data for LSA-Pro + LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalescing the short/medium summaries for LSA-Pro and long/very-long for LSA\n",
    "final_summarizer = lpro_short + lpro_med + lsa_long + lsa_super_long\n",
    "order = list(short_indices) + list(medium_indices) + list(long_indices) + list(super_long_indices)\n",
    "\n",
    "# Getting the entries in the right order\n",
    "final_sums = [i[0] for i in sorted(zip(final_summarizer,order), key=lambda pair: pair[1])]\n",
    "final_scores = evaluator.get_scores(final_sums, real_sums)\n",
    "\n",
    "\n",
    "\n",
    "randoms = [np.mean(f_rouge1), np.mean(f_rouge2), np.mean(f_rougel)]\n",
    "sb_scores = (sumbasic2_scores['rouge-1']['f'], sumbasic2_scores['rouge-2']['f'], sumbasic2_scores['rouge-l']['f'])\n",
    "tr_scores = t1_scores['rouge-1']['f'], t1_scores['rouge-2']['f'], t1_scores['rouge-l']['f']\n",
    "lr_scores = lexrank1_scores['rouge-1']['f'], lexrank1_scores['rouge-2']['f'], lexrank1_scores['rouge-l']['f']\n",
    "lsa_scores = lsa1_scores['rouge-1']['f'], lsa1_scores['rouge-2']['f'], lsa1_scores['rouge-l']['f']\n",
    "kl_scores = kl1_scores['rouge-1']['f'], kl1_scores['rouge-2']['f'], kl1_scores['rouge-l']['f']\n",
    "lsa_pro_scores2 = lsa_pro_scores['rouge-1']['f'], lsa_pro_scores['rouge-2']['f'], lsa_pro_scores['rouge-l']['f']\n",
    "final_scores2 = final_scores['rouge-1']['f'], final_scores['rouge-2']['f'], final_scores['rouge-l']['f']\n",
    "\n",
    "d7 = [randoms, sb_scores, tr_scores, kl_scores,  lr_scores,\n",
    "     lsa_scores, lsa_pro_scores2, final_scores2]\n",
    "df7 = pd.DataFrame(data=d7, columns = ['rouge-1 (%)','rouge-2 (%)', 'rouge-L (%)'], index = ['Random', 'SumBasic', 'TextRank', 'KLSum', 'LexRank', 'LSA', 'LSAPro', 'LSAPro + LSA'])\n",
    "print('Rouge F Scores for ALL Summarizers')\n",
    "round(df7, 3) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the final_sums (LSAPro + LSA) by text length\n",
    "final_short = [final_sums[i] for i in short_indices]\n",
    "final_medium = [final_sums[i] for i in medium_indices]\n",
    "final_long = [final_sums[i] for i in long_indices]\n",
    "final_super_long = [final_sums[i] for i in super_long_indices]\n",
    "\n",
    "\n",
    "# Calculating the Rouge Scores for final_sums by text length\n",
    "fin1 = evaluator.get_scores(final_short, short_sums)\n",
    "fin2 = evaluator.get_scores(final_medium, medium_sums)\n",
    "fin3 = evaluator.get_scores(final_long, long_sums)\n",
    "fin4 = evaluator.get_scores(final_super_long, super_long_sums)\n",
    "\n",
    "\n",
    "sb_by_length = [sb1['rouge-1']['f'], sb2['rouge-1']['f'], sb3['rouge-1']['f'], sb4['rouge-1']['f']]\n",
    "tr_by_length = [tr1['rouge-1']['f'], tr2['rouge-1']['f'], tr3['rouge-1']['f'], tr4['rouge-1']['f']]\n",
    "kl_by_length = [kl1['rouge-1']['f'], kl2['rouge-1']['f'], kl3['rouge-1']['f'], kl4['rouge-1']['f']]\n",
    "lex_by_length = [lex1['rouge-1']['f'], lex2['rouge-1']['f'], lex3['rouge-1']['f'], lex4['rouge-1']['f']]\n",
    "lsa_by_length = [lsa1['rouge-1']['f'], lsa2['rouge-1']['f'], lsa3['rouge-1']['f'], lsa4['rouge-1']['f']]\n",
    "lpro_by_length = [lpro1['rouge-1']['f'], lpro2['rouge-1']['f'], lpro3['rouge-1']['f'], lpro4['rouge-1']['f']]\n",
    "fin_by_length = [fin1['rouge-1']['f'], fin2['rouge-1']['f'], fin3['rouge-1']['f'], fin4['rouge-1']['f']]\n",
    "\n",
    "\n",
    "d8 = [sb_by_length, tr_by_length, kl_by_length, lex_by_length, lsa_by_length, lpro_by_length, fin_by_length]\n",
    "df8 = pd.DataFrame(data=d8, columns = ['Short','Medium', 'Long', 'Super Long'], \n",
    "                   index = ['SumBasic', 'TextRank', 'KLSum', 'LexRank', 'LSA', 'LSA-Pro', 'LSA-Pro + LSA'])\n",
    "\n",
    "print(\"Rouge-1 F Scores (%) by Text Length for ALL Summarizers\")\n",
    "round(df8, 3) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the Data Randomly into 4 Splits \n",
    "## Feel Free to adust the seed, 11 is used to match results\n",
    "## Different seeds all in general still support that (LSA-PRO + LSA) is the best summarizer\n",
    "\n",
    "import random\n",
    "random.seed(11)\n",
    "indices = list(range(50))\n",
    "ind1 = random.sample(indices, 12)\n",
    "ind2 = random.sample([i for i in indices if i not in ind1], 12)\n",
    "ind3 = random.sample([i for i in indices if i not in (ind1 + ind2)], 13)\n",
    "ind4 = random.sample([i for i in indices if i not in (ind1 + ind2 + ind3)], 13)\n",
    "\n",
    "labels = np.array(list(sums['complaint_summary']))\n",
    "inputs = np.array(list(sums['complaint_text']))\n",
    "x_split1 = list(inputs[ind1])\n",
    "x_split2 = list(inputs[ind2])\n",
    "x_split3 = list(inputs[ind3])\n",
    "x_split4 = list(inputs[ind4])\n",
    "\n",
    "y_split1 = list(labels[ind1])\n",
    "y_split2 = list(labels[ind2])\n",
    "y_split3 = list(labels[ind3])\n",
    "y_split4 = list(labels[ind4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "## Fitting the Summarizers on 4 Data Splits\n",
    "##################################################################################\n",
    "\n",
    "good_sums = list(sums[0:50]['complaint_summary'])\n",
    "s1 = [good_sums[i] for i in ind1]\n",
    "s2 = [good_sums[i] for i in ind2]\n",
    "s3 = [good_sums[i] for i in ind3]\n",
    "s4 = [good_sums[i] for i in ind4]\n",
    "\n",
    "#sumbasic\n",
    "sb_s1 = [summaries2[0:50][i] for i in ind1]\n",
    "sb_s2 = [summaries2[0:50][i] for i in ind2]\n",
    "sb_s3 = [summaries2[0:50][i] for i in ind3]\n",
    "sb_s4 = [summaries2[0:50][i] for i in ind4]\n",
    "\n",
    "# TextRank\n",
    "tr_s1 = [tsummaries1[0:50][i] for i in ind1]\n",
    "tr_s2 = [tsummaries1[0:50][i] for i in ind2]\n",
    "tr_s3 = [tsummaries1[0:50][i] for i in ind3]\n",
    "tr_s4 = [tsummaries1[0:50][i] for i in ind4]\n",
    "\n",
    "# KLSum\n",
    "l_s1 = [klsummaries1[0:50][i] for i in ind1]\n",
    "l_s2 = [klsummaries1[0:50][i] for i in ind2]\n",
    "l_s3 = [klsummaries1[0:50][i] for i in ind3]\n",
    "l_s4 = [klsummaries1[0:50][i] for i in ind4]\n",
    "\n",
    "# Lexrank\n",
    "\n",
    "lex_s1 = [lsummaries1[0:50][i] for i in ind1]\n",
    "lex_s2 = [lsummaries1[0:50][i] for i in ind2]\n",
    "lex_s3 = [lsummaries1[0:50][i] for i in ind3]\n",
    "lex_s4 = [lsummaries1[0:50][i] for i in ind4]\n",
    "\n",
    "#LSA\n",
    "lsa_s1 = [l2summaries1[0:50][i] for i in ind1]\n",
    "lsa_s2 = [l2summaries1[0:50][i] for i in ind2]\n",
    "lsa_s3 = [l2summaries1[0:50][i] for i in ind3]\n",
    "lsa_s4 = [l2summaries1[0:50][i] for i in ind4]\n",
    "\n",
    "#LSA-Pro\n",
    "lpro_s1 = [lsa_pro_sums[0:50][i] for i in ind1]\n",
    "lpro_s2 = [lsa_pro_sums[0:50][i] for i in ind2]\n",
    "lpro_s3 = [lsa_pro_sums[0:50][i] for i in ind3]\n",
    "lpro_s4 = [lsa_pro_sums[0:50][i] for i in ind4]\n",
    "\n",
    "#LSA-Pro + LSA\n",
    "final_s1 = [final_sums[0:50][i] for i in ind1]\n",
    "final_s2 = [final_sums[0:50][i] for i in ind2]\n",
    "final_s3 = [final_sums[0:50][i] for i in ind3]\n",
    "final_s4 = [final_sums[0:50][i] for i in ind4]\n",
    "\n",
    "##################################################################################\n",
    "## Obtaining Rouge Scores\n",
    "##################################################################################\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],\n",
    "                           max_n=2,\n",
    "                           apply_avg = True,\n",
    "                           stemming=True)\n",
    "\n",
    "\n",
    "sb2_s1 = evaluator.get_scores(sb_s1, s1)\n",
    "sb2_s2 = evaluator.get_scores(sb_s2, s2)\n",
    "sb2_s3 = evaluator.get_scores(sb_s3, s3)\n",
    "sb2_s4 = evaluator.get_scores(sb_s3, s4)\n",
    "\n",
    "tr2_s1 = evaluator.get_scores(tr_s1, s1)\n",
    "tr2_s2 = evaluator.get_scores(tr_s2, s2)\n",
    "tr2_s3 = evaluator.get_scores(tr_s3, s3)\n",
    "tr2_s4 = evaluator.get_scores(tr_s4, s4)\n",
    "\n",
    "kl2_s1 = evaluator.get_scores(l_s1, s1)\n",
    "kl2_s2 = evaluator.get_scores(l_s2, s2)\n",
    "kl2_s3 = evaluator.get_scores(l_s3, s3)\n",
    "kl2_s4 = evaluator.get_scores(l_s4, s4)\n",
    "\n",
    "lex2_s1 = evaluator.get_scores(lex_s1, s1)\n",
    "lex2_s2 = evaluator.get_scores(lex_s2, s2)\n",
    "lex2_s3 = evaluator.get_scores(lex_s3, s3)\n",
    "lex2_s4 = evaluator.get_scores(lex_s4, s4)\n",
    "\n",
    "lsa2_s1 = evaluator.get_scores(lsa_s1, s1)\n",
    "lsa2_s2 = evaluator.get_scores(lsa_s2, s2)\n",
    "lsa2_s3 = evaluator.get_scores(lsa_s3, s3)\n",
    "lsa2_s4 = evaluator.get_scores(lsa_s4, s4)\n",
    "\n",
    "lpro2_s1 = evaluator.get_scores(lpro_s1, s1)\n",
    "lpro2_s2 = evaluator.get_scores(lpro_s2, s2)\n",
    "lpro2_s3 = evaluator.get_scores(lpro_s3, s3)\n",
    "lpro2_s4 = evaluator.get_scores(lpro_s4, s4)\n",
    "\n",
    "final_s1 = evaluator.get_scores(final_s1, s1)\n",
    "final_s2 = evaluator.get_scores(final_s2, s2)\n",
    "final_s3 = evaluator.get_scores(final_s3, s3)\n",
    "final_s4 = evaluator.get_scores(final_s4, s4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 22: Comparing Rouge-1 Scores for Final Summarizer by Various Data Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_splits = [sb2_s1['rouge-1']['f'], sb2_s2['rouge-1']['f'], sb2_s3['rouge-1']['f'], sb2_s4['rouge-1']['f']]\n",
    "tr_splits = [tr2_s1['rouge-1']['f'], tr2_s2['rouge-1']['f'], tr2_s3['rouge-1']['f'], tr2_s4['rouge-1']['f']]\n",
    "kl_splits = [kl2_s1['rouge-1']['f'], kl2_s2['rouge-1']['f'], kl2_s3['rouge-1']['f'], kl2_s4['rouge-1']['f']]\n",
    "lex_splits = [lex2_s1['rouge-1']['f'], lex2_s2['rouge-1']['f'], lex2_s3['rouge-1']['f'], lex2_s4['rouge-1']['f']]\n",
    "lsa_splits = [lsa2_s1['rouge-1']['f'], lsa2_s2['rouge-1']['f'], lsa2_s3['rouge-1']['f'], lsa2_s4['rouge-1']['f']]\n",
    "lpro_splits = [lpro2_s1['rouge-1']['f'], lpro2_s2['rouge-1']['f'], lpro2_s3['rouge-1']['f'], lpro2_s4['rouge-1']['f']]\n",
    "final_splits = [final_s1['rouge-1']['f'], final_s2['rouge-1']['f'], final_s3['rouge-1']['f'], final_s4['rouge-1']['f']]\n",
    "\n",
    "d9 = [sb_splits, tr_splits, kl_splits, lex_splits, lsa_splits, lpro_splits, final_splits]\n",
    "df9 = pd.DataFrame(data=d9, columns = ['Split1','Split2', 'Split3', 'Split4'], \n",
    "                   index = ['SumBasic', 'TextRank', 'KLSum', 'LexRank', 'LSA', 'LSA-Pro', \"LSA-Pro + LSA\"])\n",
    "\n",
    "print(\"Rouge-1 Scores (%) by Various Data Splits\")\n",
    "round(df9, 3) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## SumBasic\n",
    "# No Stop Words\n",
    "summarizer_w_stops2 = SumBasicSummarizer()\n",
    "summaries22 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = summarizer_w_stops2(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    summaries22.append(sentence)   \n",
    "sumbasic2_scores2 = evaluator.get_scores(summaries22[26:50], real_sums[26:50])\n",
    "\n",
    "## TextRank\n",
    "tsummarizer2 = TextRankSummarizer()\n",
    "## With Stop Words and No Stemming\n",
    "tsummaries12 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = tsummarizer2(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    tsummaries12.append(sentence)\n",
    "t1_scores2 = evaluator.get_scores(tsummaries12[26:50], real_sums[26:50])\n",
    "\n",
    "## LexRank\n",
    "lsummarizer2 = LexRankSummarizer()\n",
    "## With Stop Words and No Stemming\n",
    "lsummaries12 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = lsummarizer2(parser.document, math.ceil(sent_length * 0.50))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    lsummaries12.append(sentence)\n",
    "lexrank1_scores2 = evaluator.get_scores(lsummaries12[26:50], real_sums[26:50])\n",
    "\n",
    "\n",
    "## LSA\n",
    "l2summarizer2 = LsaSummarizer()\n",
    "file = sums['complaint_text'][0]\n",
    "## With Stop Words and No Stemming\n",
    "l2summaries12 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = l2summarizer2(parser.document, math.ceil(sent_length * .50))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    l2summaries12.append(sentence)   \n",
    "lsa1_scores2 = evaluator.get_scores(l2summaries12[26:50], real_sums[26:50])\n",
    "\n",
    "## KLSum\n",
    "klsummarizer2 = KLSummarizer()\n",
    "## With Stop Words and No Stemming\n",
    "klsummaries12 = []\n",
    "for text in sums['complaint_text']:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = klsummarizer2(parser.document, math.ceil(sent_length/2))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    klsummaries12.append(sentence)\n",
    "kl1_scores2 = evaluator.get_scores(klsummaries12[26:50], real_sums[26:50])\n",
    "    \n",
    "# LSA-Pro\n",
    "trimmed = LSAPlus_SumPlus(sums['complaint_text'])\n",
    "# Applying LSA on top of the trimmed text\n",
    "lsa_pro_sums2 = []\n",
    "lsa_summarizer22 = LsaSummarizer()\n",
    "for text in trimmed:\n",
    "    sent_length = len(sent_tokenize(text))\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    summary = lsa_summarizer22(parser.document, math.ceil(sent_length * 0.50))\n",
    "    sentence = \"\"\n",
    "    for sent in summary:\n",
    "        sentence = sentence + \" \" + str(sent)\n",
    "    lsa_pro_sums2.append(sentence)    \n",
    "    \n",
    "    \n",
    "lsa_pro_scores2 = evaluator.get_scores(lsa_pro_sums[26:50], real_sums[26:50])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 23 for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sb_scores2 = (sumbasic2_scores2['rouge-1']['f'], sumbasic2_scores2['rouge-2']['f'], sumbasic2_scores2['rouge-l']['f'])\n",
    "tr_scores2 = t1_scores2['rouge-1']['f'], t1_scores2['rouge-2']['f'], t1_scores2['rouge-l']['f']\n",
    "lr_scores2 = lexrank1_scores2['rouge-1']['f'], lexrank1_scores2['rouge-2']['f'], lexrank1_scores2['rouge-l']['f']\n",
    "lsa_scores2 = lsa1_scores2['rouge-1']['f'], lsa1_scores2['rouge-2']['f'], lsa1_scores2['rouge-l']['f']\n",
    "kl_scores2 = kl1_scores2['rouge-1']['f'], kl1_scores2['rouge-2']['f'], kl1_scores2['rouge-l']['f']\n",
    "lsa_pro_scores22 = lsa_pro_scores2['rouge-1']['f'], lsa_pro_scores2['rouge-2']['f'], lsa_pro_scores2['rouge-l']['f']\n",
    "\n",
    "d0 = [sb_scores2, tr_scores2, kl_scores2,  lr_scores2,\n",
    "     lsa_scores2, lsa_pro_scores22]\n",
    "df0 = pd.DataFrame(data=d0, columns = ['rouge-1 (%)','rouge-2 (%)', 'rouge-L (%)'], index = ['SumBasic', 'TextRank', 'KLSum', 'LexRank', 'LSA', 'LSAPro'])\n",
    "print('Rouge F Scores')\n",
    "round(df0, 3) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
